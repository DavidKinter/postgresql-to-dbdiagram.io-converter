"""
DBML output generation with 2024 parser compliance.

Ensures compatibility with dbdiagram.io's January 2024 parser rebuild.
"""

from typing import Dict, List, Any, Optional
import re

class DBMLGenerator:
    """Generate DBML output from transformed schema data."""

    # PostgreSQL reserved keywords that need quoting in DBML
    RESERVED_KEYWORDS = {
        'select', 'from', 'where', 'table', 'column', 'index', 'key',
        'primary', 'foreign', 'references', 'unique', 'check', 'default',
        'null', 'not', 'and', 'or', 'in', 'exists', 'between', 'like',
        'order', 'group', 'having', 'limit', 'offset', 'join', 'left',
        'right', 'inner', 'outer', 'cross', 'union', 'except', 'intersect',
        'insert', 'update', 'delete', 'truncate', 'create', 'alter', 'drop'
    }

    def __init__(self):
        self.generated_content = []
        self.syntax_repairs = []
        self.generation_warnings = []
        self.unknown_column_counter = 0
        self.table_aliases = {}  # Store table name -> alias mapping

    def _quote_identifier(self, identifier: str) -> str:
        """Quote identifier if it contains special characters or is a reserved keyword."""
        if not identifier:
            return identifier

        # Check if already quoted
        if identifier.startswith('"') and identifier.endswith('"'):
            return identifier

        # Check if needs quoting
        needs_quoting = False

        # Check for reserved keywords (case-insensitive)
        if identifier.lower() in self.RESERVED_KEYWORDS:
            needs_quoting = True

        # Check for special characters (dots, dashes, spaces, etc.)
        # DBML only allows alphanumeric and underscores in unquoted identifiers
        if not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', identifier):
            needs_quoting = True

        # Check for Unicode characters
        try:
            identifier.encode('ascii')
        except UnicodeEncodeError:
            needs_quoting = True

        if needs_quoting:
            # Escape any quotes within the identifier
            escaped = identifier.replace('"', '\\"')
            return f'"{escaped}"'

        return identifier

    def generate(self, schema: Dict[str, Any]) -> str:
        """
        Generate complete DBML content from transformed schema.

        Follows 2024 parser requirements for syntax compliance.
        """

        self.generated_content = []
        self.syntax_repairs = []
        self.generation_warnings = []

        # Generate header comment
        self._generate_header()

        # Generate enums first
        self._generate_enums(schema.get('enums', []))

        # Generate table groups for better organization
        self._generate_table_groups(schema.get('tables', []))

        # Generate tables (with indexes inside for proper DBML syntax)
        self._generate_tables(schema.get('tables', []), schema.get('indexes', []))

        # Generate relationships
        self._generate_relationships(schema.get('relationships', []))

        # Note: Indexes are now generated inside tables for proper DBML syntax

        # Join all content
        dbml_content = '\n'.join(self.generated_content)

        # Apply final syntax compliance fixes
        dbml_content = self._apply_2024_syntax_fixes(dbml_content)

        return dbml_content

    def _generate_header(self):
        """Generate DBML file header with metadata."""

        header = [
            "// PostgreSQL to DBML Conversion",
            "// Generated by pg2dbml converter",
            "// WARNING: This is a LOSSY conversion - see report for details",
            "",
            "Project postgresql_schema {",
            "  database_type: 'PostgreSQL'",
            "  Note: '''",
            "    This DBML was automatically converted from PostgreSQL.",
            "    Some features may be missing or simplified.",
            "    See conversion report for complete details.",
            "  '''",
            "}",
            ""
        ]

        self.generated_content.extend(header)

    def _generate_enums(self, enums: List[Dict]):
        """Generate enum definitions."""
        if not enums:
            return

        for enum in enums:
            enum_name = self._quote_identifier(enum['enum_name'])

            # Start enum definition
            self.generated_content.append(f"Enum {enum_name} {{")

            # Add enum values
            for value in enum['values']:
                # Quote enum values if they contain special characters
                quoted_value = f'"{value}"' if any(c in value for c in [' ', '-', '.', '(', ')']) else value
                self.generated_content.append(f"  {quoted_value}")

            # Close enum definition
            self.generated_content.append("}")
            self.generated_content.append("")  # Empty line

    def _generate_table_groups(self, tables: List[Dict]):
        """Generate logical table groupings based on table relationships and naming patterns."""
        if not tables:
            return

        # Organize tables into groups based on naming patterns
        groups = self._organize_tables_into_groups(tables)

        for group_name, table_names in groups.items():
            if len(table_names) > 1:  # Only create groups with multiple tables
                self.generated_content.append(f'TableGroup "{group_name}" {{')
                for table_name in table_names:
                    self.generated_content.append(f'  {self._quote_identifier(table_name)}')
                self.generated_content.append("}")
                self.generated_content.append("")

    def _organize_tables_into_groups(self, tables: List[Dict]) -> Dict[str, List[str]]:
        """Organize tables into logical groups based on naming patterns and relationships."""
        groups = {}

        # Group by common prefixes
        table_names = [table['table_name'] for table in tables]

        # Find common patterns
        auth_tables = [name for name in table_names if any(keyword in name.lower()
                      for keyword in ['user', 'auth', 'login', 'session', 'account'])]

        ecommerce_tables = [name for name in table_names if any(keyword in name.lower()
                           for keyword in ['order', 'product', 'cart', 'payment', 'invoice'])]

        content_tables = [name for name in table_names if any(keyword in name.lower()
                         for keyword in ['post', 'article', 'content', 'blog', 'comment'])]

        system_tables = [name for name in table_names if any(keyword in name.lower()
                        for keyword in ['log', 'audit', 'config', 'setting', 'admin'])]

        if auth_tables:
            groups["User Management"] = auth_tables
        if ecommerce_tables:
            groups["E-commerce"] = ecommerce_tables
        if content_tables:
            groups["Content Management"] = content_tables
        if system_tables:
            groups["System & Logging"] = system_tables

        return groups

    def _generate_table_alias(self, table_name: str) -> str:
        """Generate a smart table alias for long table names."""
        # Only create aliases for very long table names (>20 chars) or complex names
        if len(table_name) <= 20:
            return None

        # Handle common patterns
        words = table_name.replace('_', ' ').split()

        if len(words) == 1:
            # Single long word - take first 3 chars + last char
            if len(table_name) > 25:
                return (table_name[:3] + table_name[-1]).upper()
        elif len(words) <= 4:
            # Multiple words - take first letter of each word
            alias = ''.join(word[0] for word in words if word).upper()
            if len(alias) >= 2:
                return alias

        # Fallback for very complex names
        return table_name[:4].upper()

    def _get_relationship_color(self, on_delete: str, on_update: str) -> str:
        """Generate intelligent relationship colors based on action types."""
        if not on_delete and not on_update:
            return None

        # Color coding based on referential actions
        if on_delete and on_delete.lower() == 'cascade':
            return "#E74C3C"  # Red for cascade (destructive)
        elif on_delete and on_delete.lower() == 'restrict':
            return "#F39C12"  # Orange for restrict (protective)
        elif on_delete and on_delete.lower() in ['set null', 'set default']:
            return "#3498DB"  # Blue for set actions (safe)
        else:
            return "#79AD51"  # Green for normal relationships

    def _generate_tables(self, tables: List[Dict], indexes: List[Dict]):
        """Generate table definitions with indexes inside."""

        for table in tables:
            self._generate_single_table(table, indexes)
            self.generated_content.append("")  # Empty line between tables

    def _generate_single_table(self, table: Dict, indexes: List[Dict]):
        """Generate a single table definition with indexes inside."""

        table_name = table['table_name']
        # Quote table name if needed
        quoted_table_name = self._quote_identifier(table_name)
        table_alias = self._generate_table_alias(table_name)
        columns = table.get('columns', [])

        # Start table definition with alias for better readability
        if table_alias:
            table_line = f"Table {quoted_table_name} as {table_alias}"
            self.table_aliases[table_name] = table_alias  # Store for relationship generation
        else:
            table_line = f"Table {quoted_table_name}"

        # Add table settings with proper spacing (2024 requirement)
        table_settings = self._generate_table_settings(table)
        if table_settings:
            table_line += f" {table_settings}"

        table_line += " {"
        self.generated_content.append(table_line)

        # Generate columns
        if columns:
            for column in columns:
                column_def = self._generate_column_definition(column, table_name)
                self.generated_content.append(f"  {column_def}")
        else:
            # DBML requires at least one column per table
            # Add a placeholder for partition/inheritance tables
            self.generated_content.append(f"  _placeholder text [note: 'Partition table - columns inherited from parent']")

        # Generate table-level constraints as notes
        table_constraints = table.get('constraints', [])
        if table_constraints:
            self._generate_table_constraints_as_notes(table_constraints, table_name)

        # Generate indexes for this table (inside table for proper DBML syntax)
        table_indexes = [idx for idx in indexes if idx.get('table_name') == table_name]
        if table_indexes:
            self.generated_content.append("")
            self.generated_content.append("  Indexes {")
            for index in table_indexes:
                index_def = self._generate_table_index(index)
                if index_def:
                    self.generated_content.append(f"    {index_def}")
            self.generated_content.append("  }")

        # Close table definition
        self.generated_content.append("}")

    def _generate_table_settings(self, table: Dict) -> str:
        """Generate table settings with 2024 parser compliance."""

        settings = []

        # Add header color for better visualization
        settings.append("headercolor: #3498DB")

        if settings:
            # 2024 parser requires space before bracket
            return f"[{', '.join(settings)}]"

        return ""

    def _generate_table_index(self, index: Dict) -> Optional[str]:
        """Generate an index definition for inside a table block with enhanced metadata."""

        columns = index.get('columns', [])
        is_unique = index.get('is_unique', False)
        index_method = index.get('index_method', 'btree')
        index_name = index.get('index_name', '')

        if not columns:
            return None

        # Clean column names and handle PostgreSQL-specific index syntax
        clean_columns = []
        for col in columns:
            cleaned_col = self._clean_index_column_reference(col)
            if cleaned_col is not None:
                clean_columns.append(cleaned_col)

        if not clean_columns:
            return None

        column_list = ", ".join(clean_columns)

        # Build index attributes list
        attributes = []

        # Add index type
        if index_method and index_method != 'btree':  # btree is default, no need to specify
            attributes.append(f"type: {index_method}")

        # Add name if present
        if index_name:
            attributes.append(f'name: "{index_name}"')

        # Generate index syntax with enhanced metadata
        if len(clean_columns) == 1:
            # Single column index
            if attributes:
                attr_str = ", ".join(attributes)
                index_def = f"{column_list} [{attr_str}]"
            else:
                index_def = f"{column_list}"
        else:
            # Multi-column index
            if attributes:
                attr_str = ", ".join(attributes)
                index_def = f"({column_list}) [{attr_str}]"
            else:
                index_def = f"({column_list})"

        # Add index type
        if is_unique:
            index_def += " [unique]"

        return index_def

    def _clean_index_column_reference(self, col: str) -> Optional[str]:
        """Clean PostgreSQL index column reference for DBML compatibility."""
        if not col:
            return None

        # Remove quotes and whitespace
        col_clean = col.strip().strip('"')

        # Handle different PostgreSQL index syntax patterns:

        # 1. Simple column with DESC/ASC: "column_name DESC" -> "column_name"
        col_clean = re.sub(r'\s+(DESC|ASC)(\s+NULLS\s+(FIRST|LAST))?\s*$', '', col_clean, flags=re.IGNORECASE)

        # 2. Column with operator class: "column_name operator_class" -> "column_name"
        # Common operator classes: gin_trgm_ops, btree_text_ops, etc.
        col_clean = re.sub(r'\s+\w+_ops\s*$', '', col_clean, flags=re.IGNORECASE)

        # 3. Expression indexes: "jsonb_column -> 'key'" -> skip these entirely
        if '->' in col_clean or '@>' in col_clean or '?' in col_clean:
            # JSON/JSONB expressions are not supported in DBML - skip them
            return None

        # 4. Function expressions: "lower(column_name)" -> try to extract base column
        function_match = re.match(r'(\w+)\s*\(\s*([^)]+)\s*\)', col_clean)
        if function_match:
            inner_content = function_match.group(2).strip()
            # For simple function(column) patterns, try to return just the column
            if re.match(r'^\w+$', inner_content):
                return self._quote_identifier(inner_content)
            else:
                # Complex function expressions - skip them
                return None

        # 5. If it's a simple column name now, quote it appropriately
        if re.match(r'^\w+$', col_clean):
            return self._quote_identifier(col_clean)

        # 6. Complex expressions - skip them entirely as DBML doesn't support them
        return None

    def _generate_column_definition(self, column: Dict, table_name: str) -> str:
        """Generate column definition with all attributes."""

        column_name = column.get('column_name', '')
        data_type = column.get('data_type', '')

        # Defensive fix: Handle missing or empty column names
        if not column_name or column_name.strip() == '' or column_name == '--':
            self.unknown_column_counter += 1
            column_name = f"unknown_column_{self.unknown_column_counter}"
            self.generation_warnings.append(
                f"Column missing name in table '{table_name}', generated as '{column_name}'"
            )

        # Defensive fix: Handle missing or empty data types
        if not data_type or data_type.strip() == '':
            data_type = 'text'
            self.generation_warnings.append(
                f"Column '{column_name}' in table '{table_name}' missing type, defaulted to 'text'"
            )

        # Quote column name if needed
        quoted_column_name = self._quote_identifier(column_name)

        # Handle special data types
        if '[]' in data_type:
            # Quote array types as required by DBML spec
            data_type = f'"{data_type}"'
        elif not re.match(r'^[a-zA-Z][a-zA-Z0-9_()]*$', data_type) and data_type:
            # Quote types with special characters
            data_type = f'"{data_type}"'

        # Build column definition
        parts = [quoted_column_name, data_type]

        # Add constraints
        constraints = []

        if not column.get('is_nullable', True):
            constraints.append("not null")

        if column.get('is_primary_key', False):
            constraints.append("primary key")

        if column.get('is_unique', False) and not column.get('is_primary_key', False):
            constraints.append("unique")

        # Add default value
        default_value = column.get('default_value')
        if default_value:
            # Ensure proper default value syntax
            formatted_default = self._format_default_value(default_value)
            constraints.append(f"default: {formatted_default}")

        # Add column note if there was a type transformation
        original_type = column.get('original_type')
        if original_type and original_type != data_type:
            # Add note inside the constraint brackets for proper DBML syntax
            note_constraint = f"note: 'Originally {original_type}'"
            constraints.append(note_constraint)

        if constraints:
            constraint_str = f"[{', '.join(constraints)}]"
            parts.append(constraint_str)

        column_def = " ".join(parts)

        return column_def

    def _format_default_value(self, default_value: str) -> str:
        """Format default value for DBML syntax compliance."""

        if not default_value:
            return "''"

        # Handle PostgreSQL type casts and complex expressions
        cleaned = default_value.strip()

        # Handle PostgreSQL type casts like '[]'::jsonb
        if '::' in cleaned:
            # Extract the value part before the type cast
            value_part = cleaned.split('::')[0].strip()
            # Remove outer quotes if present
            if (value_part.startswith("'") and value_part.endswith("'")) or \
               (value_part.startswith('"') and value_part.endswith('"')):
                value_part = value_part[1:-1]
            # Return the cleaned value in single quotes
            return f"'{value_part}'"

        # Remove outer quotes for processing
        if (cleaned.startswith("'") and cleaned.endswith("'")) or \
           (cleaned.startswith('"') and cleaned.endswith('"')):
            cleaned = cleaned[1:-1]

        # Handle different types of defaults
        if cleaned.lower() in ['true', 'false']:
            return cleaned.lower()
        elif cleaned.lower() in ['null']:
            return 'null'
        elif cleaned.upper() in ['CURRENT_TIMESTAMP', 'NOW()', 'CURRENT_DATE', 'CURRENT_TIME']:
            # PostgreSQL timestamp functions - quote them for DBML
            return f"'{cleaned}'"
        elif cleaned.startswith('`') and cleaned.endswith('`'):
            # Function calls (like UUID generation) - keep backticks
            return cleaned
        elif '(' in cleaned and ')' in cleaned and 'nextval' in cleaned.lower():
            # Handle function calls like nextval('sequence_name') - escape quotes properly
            escaped = cleaned.replace("'", "\\'")
            return f"'{escaped}'"
        elif cleaned.replace('.', '').replace('-', '').isdigit():
            # Numeric values (including decimals and negatives)
            if cleaned.startswith('-'):
                return f"'{cleaned}'"  # Quote negative numbers per 2024 parser
            return cleaned
        else:
            # String values - quote them
            return f"'{cleaned}'"

    def _generate_table_constraints_as_notes(self, constraints: List[Dict], table_name: str):
        """Generate table-level constraints as table notes."""

        constraint_notes = []

        for constraint in constraints:
            constraint_type = constraint.get('constraint_type')
            constraint_name = constraint.get('constraint_name', 'unnamed')

            if constraint_type == 'c':  # CHECK constraint
                check_expr = constraint.get('check_expression', constraint.get('definition', ''))
                constraint_notes.append(f"CHECK {constraint_name}: {check_expr}")
            elif constraint_type == 'u':  # UNIQUE constraint
                columns = constraint.get('columns', [])
                if columns:
                    constraint_notes.append(f"UNIQUE {constraint_name}: ({', '.join(columns)})")

        if constraint_notes:
            # Add table note with constraint information
            note_content = "\\n".join(constraint_notes)
            self.generated_content.append(f"  Note: '''{note_content}'''")

    def _generate_relationships(self, relationships: List[Dict]):
        """Generate relationship definitions."""

        if not relationships:
            return

        self.generated_content.append("// Relationships")

        for relationship in relationships:
            rel_def = self._generate_single_relationship(relationship)
            if rel_def:
                self.generated_content.append(rel_def)

        self.generated_content.append("")

    def _generate_single_relationship(self, relationship: Dict) -> Optional[str]:
        """Generate a single relationship definition."""

        source_table = relationship['source_table']
        source_columns = relationship['source_columns']
        target_table = relationship['target_table']
        target_columns = relationship['target_columns']

        # Handle composite relationships
        if len(source_columns) != len(target_columns):
            self._generate_warning(
                f"Relationship {source_table} -> {target_table} has mismatched column counts - skipping"
            )
            return None

        # Build relationship definition using aliases if available
        source_table_ref = self.table_aliases.get(source_table, source_table)
        target_table_ref = self.table_aliases.get(target_table, target_table)

        if len(source_columns) == 1:
            # Simple relationship
            source_ref = f"{source_table_ref}.{source_columns[0]}"
            target_ref = f"{target_table_ref}.{target_columns[0]}"
        else:
            # Composite relationship
            source_ref = f"{source_table_ref}.({', '.join(source_columns)})"
            target_ref = f"{target_table_ref}.({', '.join(target_columns)})"

        # Determine relationship type symbol
        rel_type = relationship.get('relationship_type', 'many-to-one')
        if rel_type == 'one-to-one':
            symbol = "-"
        else:
            symbol = ">"

        # Build relationship settings
        rel_settings = []
        on_delete = relationship.get('on_delete_action')
        on_update = relationship.get('on_update_action')

        if on_delete and on_delete.lower() != 'no action':
            rel_settings.append(f"delete: {on_delete.lower()}")
        if on_update and on_update.lower() != 'no action':
            rel_settings.append(f"update: {on_update.lower()}")

        # Add intelligent relationship colors based on action types
        color = self._get_relationship_color(on_delete, on_update)
        if color:
            rel_settings.append(f"color: {color}")

        # Build complete relationship definition
        if rel_settings:
            settings_str = ", ".join(rel_settings)
            rel_def = f"Ref: {source_ref} {symbol} {target_ref} [{settings_str}]"
        else:
            rel_def = f"Ref: {source_ref} {symbol} {target_ref}"

        cascade_info = []
        if on_delete == 'CASCADE':
            cascade_info.append("ON DELETE CASCADE")
        if on_update == 'CASCADE':
            cascade_info.append("ON UPDATE CASCADE")

        if cascade_info:
            rel_def += f" // {', '.join(cascade_info)} - NOT VISUALIZED IN DIAGRAM"

        return rel_def

    def _generate_indexes(self, indexes: List[Dict]):
        """Generate index definitions (if any survived transformation)."""

        if not indexes:
            return

        self.generated_content.append("// Indexes")

        for index in indexes:
            index_def = self._generate_single_index(index)
            if index_def:
                self.generated_content.append(index_def)

        self.generated_content.append("")

    def _generate_single_index(self, index: Dict) -> Optional[str]:
        """Generate a single index definition."""

        index_name = index['index_name']
        table_name = index['table_name']
        columns = index['columns']
        is_unique = index.get('is_unique', False)

        # Generate index definition
        index_type = "unique" if is_unique else "normal"
        column_list = ", ".join(columns)

        index_def = f"Indexes {{{table_name}.({column_list}) [{index_type}]}}"

        return index_def

    def _apply_2024_syntax_fixes(self, content: str) -> str:
        """Apply final syntax fixes for 2024 parser compliance."""

        # Ensure proper spacing before table settings brackets
        content = re.sub(r'(Table\s+\w+)\[', r'\1 [', content)

        # Fix line breaks before brackets (can cause errors)
        content = re.sub(r'\n\s*\[', ' [', content)

        # Ensure triple quotes for multi-line strings
        content = re.sub(r"Note:\s*'([^']*\n[^']*)'", r"Note: '''\1'''", content, flags=re.MULTILINE)

        # Remove excessive whitespace
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)

        self.syntax_repairs.append({
            'type': '2024_PARSER_COMPLIANCE',
            'description': 'Applied spacing and syntax fixes for 2024 parser'
        })

        return content

    def _generate_warning(self, message: str):
        """Generate a warning message."""
        self.generation_warnings.append({
            'type': 'GENERATION_WARNING',
            'message': message
        })

    def get_generation_report(self) -> Dict[str, Any]:
        """Get report of DBML generation process."""

        return {
            'total_lines': len(self.generated_content),
            'syntax_repairs': self.syntax_repairs,
            'warnings': self.generation_warnings,
            'generation_successful': len(self.generation_warnings) == 0
        }